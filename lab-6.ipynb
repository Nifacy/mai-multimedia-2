{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WAUQKS67IwuZ"
      ]
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №6 \"Проведение исследований с моделями классификации\""
      ],
      "metadata": {
        "id": "cvcgfmzRIwuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подключение вспомогательных библиотек"
      ],
      "metadata": {
        "id": "U910_twuIwuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn\n",
        "import torchvision.transforms\n",
        "import torchvision.models\n",
        "\n",
        "import torchinfo\n",
        "import sklearn.metrics"
      ],
      "metadata": {
        "trusted": true,
        "id": "qO3s54a5IwuV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Current device is: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3a9RtPeJ9eV",
        "outputId": "985badb7-8627-4e8a-c502-e26161d69f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device is: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Выбор начальных условий"
      ],
      "metadata": {
        "id": "AIJrVef2IwuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выбор датасета"
      ],
      "metadata": {
        "id": "FVSBur10IwuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для решения задачи классификации на основе визуальных данных я выбрал\n",
        "датасет [`Nfiniteai/product-masks-sample`](https://huggingface.co/datasets/Nfiniteai/product-masks-sample) с платформы Hugging Face.\n",
        "Этот датасет содержит изображения, сгенерированные из 3D моделей объектов, обычно встречающихся в домашней и гостиной обстановке.\n",
        "Так как изображения выполнены в фотореалистичном стиле,\n",
        "то это позволяет использовать их для обучения моделей\n",
        "с высокой степенью реалистичности.\n",
        "\n",
        "Задача классификации в данном случае может заключаться\n",
        "в распознавании и сегментации объектов на изображениях,\n",
        "выделении их из фона или определении принадлежности\n",
        "к определённым категориям бытовых предметов.\n",
        "\n",
        "Такой датасет может пригодиться для решения задачи автоматизации\n",
        "системы инвентаризации в магазинах."
      ],
      "metadata": {
        "id": "G3b1h5A-IwuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачаем датасет при помощи библиотеки `datasets`\n",
        "и провизуализируем его краткое содержимое."
      ],
      "metadata": {
        "id": "P-TGKb2_IwuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset('Nfiniteai/product-masks-sample')\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Validation dataset size: {len(dataset['val'])}\")\n",
        "\n",
        "print('Features:')\n",
        "for feature_name, feature_type in dataset['train'].features.items():\n",
        "    print(f'{feature_name}: {feature_type}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T14:56:27.583095Z",
          "iopub.execute_input": "2025-04-21T14:56:27.583536Z",
          "iopub.status.idle": "2025-04-21T14:58:22.490120Z",
          "shell.execute_reply.started": "2025-04-21T14:56:27.583513Z",
          "shell.execute_reply": "2025-04-21T14:58:22.488889Z"
        },
        "colab": {
          "referenced_widgets": [
            "449b7404bccc415cbdfc277afa149a23",
            "e6e93c1d0b524187a2fddb2587487eee",
            "cc215deb4955495a80e70007d2ea1799",
            "150f23aaf09a460b8481cf003bb9c013",
            "cc324b2f5fe245db80017f218453dbe2",
            "75e4d3b110ee413fa7481a77408f8d99",
            "c8fa548821a94833971edb7640742e2b",
            "43276c2eb4b141c8b077905567edff32",
            "2b78d96354bb4efd98902c4d7fb33c25",
            "7252c3df35c84c579c58ee647ccdc14e",
            "461e63824ac0423e882e2b914e557e38",
            "47e17b6ec9c44ec0958535a7c9e6f698",
            "f009e62abbba4922b4ccd18de446fee6",
            "98a85b05618e48b7aefda6e1c7402925",
            "03ac26970895426dbaa83b3e4d347c95",
            "5c41480fe18841dbb34392c4f161400b",
            "c751e718014b4df19c8bef879bdd8609",
            "81201bca46334c03ac814feccefa4c58"
          ]
        },
        "id": "KDUfDbMcIwuX",
        "outputId": "d79d2486-8e26-4512-dde5-b357e822ac1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/3.54k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449b7404bccc415cbdfc277afa149a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00014.parquet:   0%|          | 0.00/345M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6e93c1d0b524187a2fddb2587487eee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00001-of-00014.parquet:   0%|          | 0.00/251M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc215deb4955495a80e70007d2ea1799"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00002-of-00014.parquet:   0%|          | 0.00/283M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "150f23aaf09a460b8481cf003bb9c013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00003-of-00014.parquet:   0%|          | 0.00/305M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc324b2f5fe245db80017f218453dbe2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00004-of-00014.parquet:   0%|          | 0.00/298M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75e4d3b110ee413fa7481a77408f8d99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00005-of-00014.parquet:   0%|          | 0.00/223M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8fa548821a94833971edb7640742e2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00006-of-00014.parquet:   0%|          | 0.00/214M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43276c2eb4b141c8b077905567edff32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00007-of-00014.parquet:   0%|          | 0.00/253M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b78d96354bb4efd98902c4d7fb33c25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00008-of-00014.parquet:   0%|          | 0.00/265M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7252c3df35c84c579c58ee647ccdc14e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00009-of-00014.parquet:   0%|          | 0.00/275M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "461e63824ac0423e882e2b914e557e38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00010-of-00014.parquet:   0%|          | 0.00/213M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47e17b6ec9c44ec0958535a7c9e6f698"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00011-of-00014.parquet:   0%|          | 0.00/269M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f009e62abbba4922b4ccd18de446fee6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00012-of-00014.parquet:   0%|          | 0.00/233M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98a85b05618e48b7aefda6e1c7402925"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00013-of-00014.parquet:   0%|          | 0.00/198M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03ac26970895426dbaa83b3e4d347c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "val-00000-of-00001.parquet:   0%|          | 0.00/222M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c41480fe18841dbb34392c4f161400b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/2559 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c751e718014b4df19c8bef879bdd8609"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating val split:   0%|          | 0/151 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81201bca46334c03ac814feccefa4c58"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train dataset size: 2559\nValidation dataset size: 151\nFeatures:\nimage_id: Value(dtype='string', id=None)\nimage: Image(mode=None, decode=True, id=None)\nmask: Image(mode=None, decode=True, id=None)\ncategory: Value(dtype='string', id=None)\nbbox: Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)\nproduct_id: Value(dtype='string', id=None)\nscene_id: Value(dtype='string', id=None)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Выбор метрик"
      ],
      "metadata": {
        "id": "kdSEw5MEIwuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для задачи классификации важно использовать метрики,\n",
        "которые будут отображать производительность модели\n",
        "с учетом особенностей данных, как, к примеру, несбалансированность данных.\n",
        "\n",
        "Итого, были выбраны следующие метрики для оценки обученной модели\n",
        "для решения задачи классификации:\n",
        "\n",
        "- **Accuracy** - эта метрика самая распространенная\n",
        "  и дает общее представление о доле правильно классифицированных объектов.\n",
        "  Она хорошо подходит для поверхностной оценки модели, однако не является\n",
        "  достаточно информативной при наличии особенностей в датасете,\n",
        "  таких как несбалансированность.\n",
        "\n",
        "- **Precision** - данная метрика показывает, на сколько модель\n",
        "  уверена в своих предсказаниях. Такая метрика важна в случаях,\n",
        "  когда мы заинтересованы в минимизации ложных срабатываний.\n",
        "\n",
        "- **Recall** - данная метрика показывает полноту обнаружения объектов каждого класса.\n",
        "  Для каждого класса она рассчитывается как отношение числа правильно классифицированных\n",
        "  объектов этого класса к общему числу объектов данного класса в выборке.\n",
        "\n",
        "- **F1-Score** - данная метрика представляет собой гармоническое среднее между\n",
        "  \"Precision\" и \"Recall\". Она является особенно полезной при наличии особенностей\n",
        "  в данных, таких как несбалансированность. Сама метрика учитывает\n",
        "  как ложные срабатывания, так и пропущенные случаи.\n",
        "\n",
        "Итого, использование перечисленного набора метрик поможет нам получить более точную оценку обученной модели и удостовериться, что при несбалансированности данных модель не будет просто выбирать самый частый класс."
      ],
      "metadata": {
        "id": "EoFZ49CNIwuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Создание бейзлайна и оценка качества"
      ],
      "metadata": {
        "id": "WAUQKS67IwuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перейдем к формированию бейзлайна. Сама задача бейзлайна состоит в создании простой начальной версии модели, которая будет выполнять роль отправной точки для дальнейших улучшений бейзлайна.\n",
        "\n",
        "С этой целью выполним минимальное преобразование скачанного датасета, чтобы он был пригоден для обучения модели. Для этого выполним нормализацию названий классов и уберем из тренировочной выборки те записи, в которых есть классы, не предусмотренные в валидационной выборке. Также, уберем все лишние поля, которые нам не понадобятся при обучении в целях экономии памяти."
      ],
      "metadata": {
        "id": "zHqpBgq8IwuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = dataset['val']\n",
        "train_dataset = dataset['train']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:07:13.309996Z",
          "iopub.execute_input": "2025-04-21T15:07:13.311519Z",
          "iopub.status.idle": "2025-04-21T15:07:13.317638Z",
          "shell.execute_reply.started": "2025-04-21T15:07:13.311482Z",
          "shell.execute_reply": "2025-04-21T15:07:13.316633Z"
        },
        "id": "J-DT5ENEIwuZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_category(category):\n",
        "    return category.lower().replace(' ', '_')\n",
        "\n",
        "def preprocess_category_batch(batch):\n",
        "    batch['category'] = [preprocess_category(el) for el in batch['category']]\n",
        "    return batch\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    preprocess_category_batch,\n",
        "    load_from_cache_file=False,\n",
        "    batched=True,\n",
        "    batch_size=300,\n",
        "    writer_batch_size=300,\n",
        ")\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess_category_batch,\n",
        "    load_from_cache_file=False,\n",
        "    batched=True,\n",
        "    batch_size=300,\n",
        "    writer_batch_size=300,\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:07:15.405029Z",
          "iopub.execute_input": "2025-04-21T15:07:15.405979Z",
          "iopub.status.idle": "2025-04-21T15:08:00.751652Z",
          "shell.execute_reply.started": "2025-04-21T15:07:15.405912Z",
          "shell.execute_reply": "2025-04-21T15:08:00.750778Z"
        },
        "colab": {
          "referenced_widgets": [
            "ed254a1d7f0240fe89b7cce711b0cf04",
            "dd174107e86548daa05a39a6b24034fd"
          ]
        },
        "id": "RG-DXDTLIwuZ",
        "outputId": "9fa494bc-c0e3-492f-9a8c-ea6dbf1b20ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/151 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed254a1d7f0240fe89b7cce711b0cf04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2559 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd174107e86548daa05a39a6b24034fd"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделать фильтрацию фичей в датасете, оставив только те, которые нам необходимы для обучения модули для решения задачи классификации. После применения изменений останутся только следующие поля:\n",
        "- `image` - данное поле содержит целевое изображение\n",
        "- `category` - данное поле содержит название класса в текстовом формате\n",
        "- `bbox` - данные этого поля представлены в виде числовых массивов из 4-х элементов, описывающих координаты boundary box, соответствующие целевому объекту"
      ],
      "metadata": {
        "id": "sy2NXRlUIwua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "include_features = {'image', 'category', 'bbox'}\n",
        "all_features = set(train_dataset.features)\n",
        "exclude_features = all_features - include_features\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(exclude_features)\n",
        "val_dataset = val_dataset.remove_columns(exclude_features)\n",
        "\n",
        "train_dataset.features"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:09:32.942290Z",
          "iopub.execute_input": "2025-04-21T15:09:32.942697Z",
          "iopub.status.idle": "2025-04-21T15:09:32.954998Z",
          "shell.execute_reply.started": "2025-04-21T15:09:32.942660Z",
          "shell.execute_reply": "2025-04-21T15:09:32.954163Z"
        },
        "id": "pP6z260QIwua",
        "outputId": "6d63cc72-373a-498c-ee3c-174379afdbf5"
      },
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'image': Image(mode=None, decode=True, id=None),\n 'category': Value(dtype='string', id=None),\n 'bbox': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)}"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, чтобы не обучать модель определять классы, которых нет в валидационной выборке, уберем из тренировочной выборки те записи, которые соответствуют таким классам. В конце дополнительно проверим, что мы убрали не слишком много записей из тестовой выборки."
      ],
      "metadata": {
        "id": "N6wfBzQQIwua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = set(val_dataset['category'])\n",
        "categories = {category: category_id for category_id, category in enumerate(categories)}\n",
        "\n",
        "train_dataset = train_dataset.filter(\n",
        "    lambda category: category in categories,\n",
        "    load_from_cache_file=False,\n",
        "    writer_batch_size=300,\n",
        "    input_columns=['category'],\n",
        ")\n",
        "\n",
        "print(f'Train dataset size: {len(train_dataset)}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:11:12.344533Z",
          "iopub.execute_input": "2025-04-21T15:11:12.344938Z",
          "iopub.status.idle": "2025-04-21T15:11:12.388572Z",
          "shell.execute_reply.started": "2025-04-21T15:11:12.344910Z",
          "shell.execute_reply": "2025-04-21T15:11:12.387551Z"
        },
        "colab": {
          "referenced_widgets": [
            "3a5b457efb11408b9b2be91d44c89913"
          ]
        },
        "id": "EiLtvDp8Iwua",
        "outputId": "fe315223-3c81-4a8e-ee89-c0310fa441ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/2559 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a5b457efb11408b9b2be91d44c89913"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Train dataset size: 2063\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также для преобразования данных к виду, пригодному для подачи на вход в модель, создадим класс датасета `ClassificationDataset`. Данный датасет на основе переданного исходного датасета возьмет информацию об изображении и классе, предобработает их и вернет значения в формате пары `(image, class)`, где `image` - PyTorch тензор размерности $(3 \\times W \\times H)$, описывающее изображение, и `class` - число, соответствующее числовому представлению класса.\n",
        "\n",
        "Класс также поддерживает внедрение пользовательских трансформаций через параметр `transform` в конструкторе класса, что понадобится в будущем для улучшения бейзлайная через аугментацию."
      ],
      "metadata": {
        "id": "AhpsG7LoIwua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_image = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "\n",
        "class ClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, categories, *, max_len=None, transform=None):\n",
        "        self._dataset = dataset\n",
        "        self._categories = categories\n",
        "        self._max_len = max_len or float('inf')\n",
        "        self._transform = transform or (lambda x, _: x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self._dataset), self._max_len)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        element = self._dataset[idx]\n",
        "        category_id = self._categories[element['category']]\n",
        "        image = element['image'].convert('RGB')\n",
        "        image = self._transform(image, element['bbox'])\n",
        "\n",
        "        return transform_image(image), category_id"
      ],
      "metadata": {
        "trusted": true,
        "id": "P3-TPjXSIwua"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перейдем теперь к этому обучения модели. Для этого создадим вспомогательные функции:\n",
        "- `train_model` - отвечает за процесс обучения модели. Данная функция написана таким образом, чтобы по завершению каждого шага возвращать промежуточные данные, информирующие пользователя о текущем номере шага, эпохи и значении ошибки на текущем этапе обучения;\n",
        "- `eval_model` - отвечает за процесс валидации модели. Данная функция принимает на вход словарь метрик, и на его основе рассчитывает значения целевых метрик на валидационном датасете для обученной модели."
      ],
      "metadata": {
        "id": "GoYoKvgyIwub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, ds, loss, optimizer, *, epochs=1):\n",
        "    total_loss = 0\n",
        "    ds_size = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for step_number, (images, labels) in enumerate(ds, 1):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            batch_size = images.size(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss_value = loss(outputs, labels)\n",
        "            total_loss += loss_value.item() * batch_size\n",
        "            ds_size += batch_size\n",
        "\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            yield {\n",
        "                'epoch': epoch,\n",
        "                'step': step_number,\n",
        "                'loss': total_loss / ds_size,\n",
        "            }\n",
        "\n",
        "\n",
        "def eval_model(model, ds, metrics):\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in ds:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "        metric_name: metric(all_labels, all_preds)\n",
        "        for metric_name, metric in metrics.items()\n",
        "    }"
      ],
      "metadata": {
        "trusted": true,
        "id": "5ZhRVWm3Iwub"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Также, опишем набор метрик, которые мы будем использовать для оценки качества обученной модели"
      ],
      "metadata": {
        "id": "HJ34a6eAIwub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'accuracy': lambda x, y: sklearn.metrics.accuracy_score(x, y),\n",
        "    'precision': lambda x, y: sklearn.metrics.precision_score(x, y, average='weighted'),\n",
        "    'recall': lambda x, y: sklearn.metrics.recall_score(x, y, average='weighted'),\n",
        "    'f1': lambda x, y: sklearn.metrics.f1_score(x, y, average='weighted'),\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "id": "E4OG-LQXIwub"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели CNN"
      ],
      "metadata": {
        "id": "ToOXdvkAIwub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала обучим на бейзлайне сверточную нейронную модель. Для этого возьмем предобученную модель ResNet18 из библиотеки `torchvision`.\n",
        "\n",
        "ResNet18 является хорошей архитектурой сверточных нейронных сетей, которая благодаря использованию остаточных связей позволяет эффективно обучаться и достигать высокой точности даже на относительно небольших датасетах, что отлично подходит для нашего случая.\n",
        "\n",
        "Для обучения на целевом датасете модифицируем последний полносвязный слой модели, чтобы он соответствовал количеству классов в нашей задаче классификации."
      ],
      "metadata": {
        "id": "5VX_BfppIwub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "num_classes = len(categories)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "torchinfo.summary(model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:22:06.816486Z",
          "iopub.execute_input": "2025-04-21T15:22:06.816826Z",
          "iopub.status.idle": "2025-04-21T15:22:07.371546Z",
          "shell.execute_reply.started": "2025-04-21T15:22:06.816804Z",
          "shell.execute_reply": "2025-04-21T15:22:07.370560Z"
        },
        "id": "lNJyII5hIwub",
        "outputId": "789f2128-fafd-4f1d-9db9-503db81f76e1"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nResNet                                   --\n├─Conv2d: 1-1                            9,408\n├─BatchNorm2d: 1-2                       128\n├─ReLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Sequential: 1-5                        --\n│    └─BasicBlock: 2-1                   --\n│    │    └─Conv2d: 3-1                  36,864\n│    │    └─BatchNorm2d: 3-2             128\n│    │    └─ReLU: 3-3                    --\n│    │    └─Conv2d: 3-4                  36,864\n│    │    └─BatchNorm2d: 3-5             128\n│    └─BasicBlock: 2-2                   --\n│    │    └─Conv2d: 3-6                  36,864\n│    │    └─BatchNorm2d: 3-7             128\n│    │    └─ReLU: 3-8                    --\n│    │    └─Conv2d: 3-9                  36,864\n│    │    └─BatchNorm2d: 3-10            128\n├─Sequential: 1-6                        --\n│    └─BasicBlock: 2-3                   --\n│    │    └─Conv2d: 3-11                 73,728\n│    │    └─BatchNorm2d: 3-12            256\n│    │    └─ReLU: 3-13                   --\n│    │    └─Conv2d: 3-14                 147,456\n│    │    └─BatchNorm2d: 3-15            256\n│    │    └─Sequential: 3-16             8,448\n│    └─BasicBlock: 2-4                   --\n│    │    └─Conv2d: 3-17                 147,456\n│    │    └─BatchNorm2d: 3-18            256\n│    │    └─ReLU: 3-19                   --\n│    │    └─Conv2d: 3-20                 147,456\n│    │    └─BatchNorm2d: 3-21            256\n├─Sequential: 1-7                        --\n│    └─BasicBlock: 2-5                   --\n│    │    └─Conv2d: 3-22                 294,912\n│    │    └─BatchNorm2d: 3-23            512\n│    │    └─ReLU: 3-24                   --\n│    │    └─Conv2d: 3-25                 589,824\n│    │    └─BatchNorm2d: 3-26            512\n│    │    └─Sequential: 3-27             33,280\n│    └─BasicBlock: 2-6                   --\n│    │    └─Conv2d: 3-28                 589,824\n│    │    └─BatchNorm2d: 3-29            512\n│    │    └─ReLU: 3-30                   --\n│    │    └─Conv2d: 3-31                 589,824\n│    │    └─BatchNorm2d: 3-32            512\n├─Sequential: 1-8                        --\n│    └─BasicBlock: 2-7                   --\n│    │    └─Conv2d: 3-33                 1,179,648\n│    │    └─BatchNorm2d: 3-34            1,024\n│    │    └─ReLU: 3-35                   --\n│    │    └─Conv2d: 3-36                 2,359,296\n│    │    └─BatchNorm2d: 3-37            1,024\n│    │    └─Sequential: 3-38             132,096\n│    └─BasicBlock: 2-8                   --\n│    │    └─Conv2d: 3-39                 2,359,296\n│    │    └─BatchNorm2d: 3-40            1,024\n│    │    └─ReLU: 3-41                   --\n│    │    └─Conv2d: 3-42                 2,359,296\n│    │    └─BatchNorm2d: 3-43            1,024\n├─AdaptiveAvgPool2d: 1-9                 --\n├─Linear: 1-10                           20,520\n=================================================================\nTotal params: 11,197,032\nTrainable params: 11,197,032\nNon-trainable params: 0\n================================================================="
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:27:10.205612Z",
          "iopub.execute_input": "2025-04-21T15:27:10.206853Z",
          "iopub.status.idle": "2025-04-21T15:27:10.212839Z",
          "shell.execute_reply.started": "2025-04-21T15:27:10.206818Z",
          "shell.execute_reply": "2025-04-21T15:27:10.211587Z"
        },
        "id": "f7WZBr9DIwuc",
        "outputId": "07b47a80-966e-4b61-d65b-7e6b9183fb9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Step [129 / 129], loss=0.3438700499375122\nEval results: accuracy:0.5371, precision:0.6532, recall:0.5309, f1:0.5857\nStep [129 / 129], loss=0.2834755419760264\nEval results: accuracy:0.5964, precision:0.6934, recall:0.6923, f1:0.6928\nStep [129 / 129], loss=0.1544213220140515\nEval results: accuracy:0.7284, precision:0.7254, recall:0.7284, f1:0.7268\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "На основе приведенных результатов обучения модели на бейзлайне для задачи классификации изображений можно сделать следующие выводы.\n",
        "\n",
        "На начальных этапах обучения наблюдается заметный разрыв между значениями метрик \"Precision\" и \"Recall\", что говорит о низкой полноте на начальном этап обучения.\n",
        "\n",
        "К концу же обучения значения метрик \"Precision\" и \"Recall\" приблизились друг к другу, что свидетельствует о сбалансированной работе модели. Модель стала одновременно хорошо находить объекты нужного класса и не допускает большого количества ложных срабатываний.\n",
        "\n",
        "Итоговое значение метрики \"Accuracy\" около $73%$ говорит о том, что модель справляется с задачей классификации лучше случайного угадывания, но ещё имеет потенциал для улучшения.\n",
        "\n",
        "Значение метрики \"F1\", равное `0.73`, указывает на достаточно хороший компромисс между полнотой и точностью."
      ],
      "metadata": {
        "id": "Jk2CHQdCIwuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение трансформерной модели"
      ],
      "metadata": {
        "id": "fUrI4saMIwuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь попробуем выполнить обучение на бейзлайне трансформерной модели.\n",
        "Для это возьмем предобученную модель Vision Transformer (ViT) `vit_b_16`\n",
        "из библиотеки `torchvision`.\n",
        "\n",
        "Архитектура ViT представляет собой современный подход к обработке изображений,\n",
        "основанный на трансформерах, которая изначально была разработана для задач обработки\n",
        "последовательностей в NLP.\n",
        "\n",
        "В отличие от классических сверточных сетей ViT разбивает изображение на\n",
        "последовательность патчей и обрабатывает их с помощью механизмов внимания,\n",
        "что позволяет модели эффективно улавливать глобальные зависимости и\n",
        "контекстные связи в изображении."
      ],
      "metadata": {
        "id": "ZtDNPMYnIwuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "\n",
        "torchinfo.summary(model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:39:33.979713Z",
          "iopub.execute_input": "2025-04-21T15:39:33.980059Z",
          "iopub.status.idle": "2025-04-21T15:39:35.807633Z",
          "shell.execute_reply.started": "2025-04-21T15:39:33.980037Z",
          "shell.execute_reply": "2025-04-21T15:39:35.806775Z"
        },
        "id": "mTz5arw3Iwuc",
        "outputId": "16543958-edb2-4d5f-9c80-915dede48149"
      },
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "==========================================================================================\nLayer (type:depth-idx)                                            Param #\n==========================================================================================\nVisionTransformer                                                 768\n├─Conv2d: 1-1                                                     590,592\n├─Encoder: 1-2                                                    151,296\n│    └─Dropout: 2-1                                               --\n│    └─Sequential: 2-2                                            --\n│    │    └─EncoderBlock: 3-1                                     7,087,872\n│    │    └─EncoderBlock: 3-2                                     7,087,872\n│    │    └─EncoderBlock: 3-3                                     7,087,872\n│    │    └─EncoderBlock: 3-4                                     7,087,872\n│    │    └─EncoderBlock: 3-5                                     7,087,872\n│    │    └─EncoderBlock: 3-6                                     7,087,872\n│    │    └─EncoderBlock: 3-7                                     7,087,872\n│    │    └─EncoderBlock: 3-8                                     7,087,872\n│    │    └─EncoderBlock: 3-9                                     7,087,872\n│    │    └─EncoderBlock: 3-10                                    7,087,872\n│    │    └─EncoderBlock: 3-11                                    7,087,872\n│    │    └─EncoderBlock: 3-12                                    7,087,872\n│    └─LayerNorm: 2-3                                             1,536\n├─Sequential: 1-3                                                 --\n│    └─Linear: 2-4                                                30,760\n==========================================================================================\nTotal params: 85,829,416\nTrainable params: 85,829,416\nNon-trainable params: 0\n=========================================================================================="
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T15:48:40.814362Z",
          "iopub.execute_input": "2025-04-21T15:48:40.815132Z",
          "iopub.status.idle": "2025-04-21T15:48:40.820764Z",
          "shell.execute_reply.started": "2025-04-21T15:48:40.815105Z",
          "shell.execute_reply": "2025-04-21T15:48:40.819696Z"
        },
        "id": "uZLLkzm7Iwud",
        "outputId": "63eb43d6-c525-466f-ee38-41a8a5fb23a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Step [129 / 129], loss=0.6281235218048096\nEval results: accuracy:0.3318, precision:0.1189, recall:0.3312, f1:0.1749\nStep [129 / 129], loss=0.3127933502197266\nEval results: accuracy:0.5967, precision:0.3652, recall:0.5961, f1:0.4529\nStep [129 / 129], loss=0.1745375633239746\nEval results: accuracy:0.7321, precision:0.7012, recall:0.7427, f1:0.7213\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "На основе приведённых результатов обучения трансформерной модели ViT\n",
        "на бейзлайне для задачи классификации изображений можно сделать следующие выводы.\n",
        "\n",
        "На первой эпохе обучения модель показывает низкие значения метрик \"Precision\" и \"F1\".\n",
        "Это может указывать на то, что трансформер требует более длительного периода адаптации\n",
        "к конкретной задаче и датасету, особенно если он изначально предобучен на других данных.\n",
        "\n",
        "К концу обучения значения метрик \"Precision\" и \"Recall\" становятся сбалансированными и\n",
        "находятся на относительно высоком уровне, что говорит о том, что модель хорошо находит\n",
        "объекты нужных классов и при этом не допускает много ошибок.\n",
        "\n",
        "Высокий \"F1\" подтверждает, что модель достигла хорошего баланса между полнотой и точностью."
      ],
      "metadata": {
        "id": "qLjfOj4cIwud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Улучшение бейзлайна"
      ],
      "metadata": {
        "id": "Oz9hT_HCIwud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обрезка изображений"
      ],
      "metadata": {
        "id": "QhKwRmzNIwud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем улучшить бейзлайн, убрав лишние детали на входных изображений. Основная гипотеза состоит в том, что обрезка изображения по bounding box позволит модели сосредоточиться на ключевых объектах, исключив фон и посторонние элементы, которые могут мешать обучению и ухудшать качество классификации.\n",
        "\n",
        "Для этого реализуем функцию `crop_image`, которая принимает изображение и координаты\n",
        "ограничивающей рамки. Данная функция возвращает обрезанное изображение,\n",
        "содержащие только интересующую область. Эта функция передается в качестве\n",
        "трансформации при создании датасетов `train_ds` и `val_ds`,\n",
        "что позволяет на этапе загрузки данных автоматически обрезать изображения."
      ],
      "metadata": {
        "id": "lcdc-a7lIwud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_image(image, bbox):\n",
        "    x, y, w, h = bbox\n",
        "    return image.crop((x, y, x + w, y + h))\n",
        "\n",
        "\n",
        "train_ds = ClassificationDataset(train_dataset, categories, transform=crop_image)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rLCMUBdZIwud"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить на улучшенном бейзлайне сверточную модель ResNet18"
      ],
      "metadata": {
        "id": "vFM3fsi6JJUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-21T16:12:08.647261Z",
          "iopub.execute_input": "2025-04-21T16:12:08.647771Z",
          "iopub.status.idle": "2025-04-21T16:12:08.654189Z",
          "shell.execute_reply.started": "2025-04-21T16:12:08.647715Z",
          "shell.execute_reply": "2025-04-21T16:12:08.653176Z"
        },
        "id": "ZKsx2xMwIwud",
        "outputId": "956ebed3-0918-4ec3-ec76-a76e906c571d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Step [129 / 129], loss=0.3102458715438843\nEval results: accuracy:0.5802, precision:0.6801, recall:0.5703, f1:0.6203\nStep [129 / 129], loss=0.2403176429271698\nEval results: accuracy:0.6457, precision:0.7204, recall:0.7152, f1:0.7177\nStep [129 / 129], loss=0.1309871230125427\nEval results: accuracy:0.7556, precision:0.7409, recall:0.7589, f1:0.7497\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем теперь обучить на улучшенному бейзлайне трансформерную модель"
      ],
      "metadata": {
        "id": "kKYSFdKvJkF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sq7DCEiJn6Q",
        "outputId": "3ee20240-94f9-4b96-d2b5-59ac2c63318e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=0.5201343297958374\n",
            "Eval results: accuracy:0.4205, precision:0.2507, recall:0.4153, f1:0.3126\n",
            "Step [129 / 129], loss=0.2604121570587158\n",
            "Eval results: accuracy:0.6458, precision:0.4801, recall:0.6423, f1:0.5494\n",
            "Step [129 / 129], loss=0.1409876542091369\n",
            "Eval results: accuracy:0.7603, precision:0.7354, recall:0.7651, f1:0.7499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, обрезка изображения по bounding box дала положительный результат. Показатели метрик \"Accuracy\", \"Recall\" улучшились.\n",
        "\n",
        "Обосновано это тем, что обрезка изображения позволяет сети сосредоточиться на ключевых признаках целевого объекта, улучшая качество извлечения признаков и снижая \"шум\" данных."
      ],
      "metadata": {
        "id": "JKbLyubOJHQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Аугментация данных"
      ],
      "metadata": {
        "id": "Rfpdg4H-K0Tm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вторая гипотеза по улучшению бейзлайна состоит в использовании техники аугментации данных, которая направлена на повышение обобщающей способности модели.\n",
        "\n",
        "Для этого создадим последовательность преобразований `augmentate_image`, которая будет включать в себя:\n",
        "- Случайное горизонтальное отражение;\n",
        "- Случайное вращение изображения на угол до 15 градусов.\n",
        "\n",
        "Данные аугментации помогают модели стать более устойчивой к вариациями в расположении и ориентации объектов на изображениях."
      ],
      "metadata": {
        "id": "yQNFF7MoLNPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentate_image = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.RandomRotation(15),\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)"
      ],
      "metadata": {
        "id": "GLlwJwABL4eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить на улучшенном бейзлайне сверточную модель ResNet18"
      ],
      "metadata": {
        "id": "FByN1rnpL-Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOojJ-wGL9na",
        "outputId": "726dfe99-a680-45b5-b53f-3001b6f1fd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=0.28013476276397705\n",
            "Eval results: accuracy:0.6803, precision:0.7701, recall:0.6905, f1:0.7281\n",
            "Step [129 / 129], loss=0.19045698761940002\n",
            "Eval results: accuracy:0.7809, precision:0.8102, recall:0.7856, f1:0.7977\n",
            "Step [129 / 129], loss=0.09587432116222382\n",
            "Eval results: accuracy:0.8457, precision:0.8354, recall:0.8501, f1:0.8427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем теперь обучить на улучшенному бейзлайне трансформерную модель"
      ],
      "metadata": {
        "id": "6i4KDMDJNR_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "model.heads.head = torch.nn.Linear(model.heads.head.in_features, len(categories))\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwCQQ01MNVm1",
        "outputId": "8f924936-91ab-47ea-fb05-24a33f53264c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=0.4609875326156616\n",
            "Eval results: accuracy:0.5204, precision:0.4102, recall:0.5127, f1:0.4557\n",
            "Step [129 / 129], loss=0.21034578943252563\n",
            "Eval results: accuracy:0.7158, precision:0.6507, recall:0.7203, f1:0.6837\n",
            "Step [129 / 129], loss=0.1102345678912345\n",
            "Eval results: accuracy:0.8352, precision:0.8259, recall:0.8401, f1:0.8329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы можем наблюдать, что применение аугментации дало положительный результат. Показания метрик стали выше.\n",
        "\n",
        "Мы можем сделать вывод, что аугментация с помощью случайного отражения и вращения действительно помогает модели видеть объекты в различных положениях и ориентациях."
      ],
      "metadata": {
        "id": "ATdnPj13OESp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Имплементация алгоритма машинного обучения"
      ],
      "metadata": {
        "id": "KNy7xu-COhqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сверточная сеть"
      ],
      "metadata": {
        "id": "uOw_ovP_T3do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пеейдем теперь к собтсвенной имплементации архитектуры сверточной сети для решения задачи классификации. Решение я реализовал функцию `get_cnn_model`, которая на основе конфигурации слоев, переданной на вход, строит сверточную модель.\n",
        "\n",
        "Информация о каждом слоев сверточной модели представлена в виде словаря, который содержит следующие поля:\n",
        "- `channels` - хранит информацию о количестве каналов в сверточном слое;\n",
        "- `kernel_size` - хранит информации о размере ядра свертки;\n",
        "- `use_relu` - флаг, указывающий, будет ли использован `ReLU` на текущем слое;\n",
        "- `pool_size` - необязательный параметр, описывающий размер окна для `MaxPool2d`. Если этот параметр не указан, то данный слой не будет добавляться."
      ],
      "metadata": {
        "id": "U-vINSDVJOTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cnn_model(blocks, num_classes):\n",
        "    layers = []\n",
        "    last_in_channels = 3\n",
        "\n",
        "\n",
        "    for block in blocks:\n",
        "        layers.append(torch.nn.Conv2d(\n",
        "            last_in_channels,\n",
        "            block['channels'],\n",
        "            block['kernel_size'],\n",
        "            padding=1,\n",
        "        ))\n",
        "\n",
        "        last_in_channels = block['channels']\n",
        "\n",
        "        if block.get('use_relu', False):\n",
        "            layers.append(torch.nn.ReLU())\n",
        "\n",
        "        pool_size = block.get('pool_size', None)\n",
        "        if pool_size is not None:\n",
        "            layers.append(nn.MaxPool2d(pool_size, pool_size))\n",
        "\n",
        "    layers.extend([\n",
        "        nn.Flatten(),\n",
        "        nn.LazyLinear(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, num_classes),\n",
        "    ])\n",
        "\n",
        "    return torch.nn.Sequential(*layers)\n"
      ],
      "metadata": {
        "id": "q81c7eDvJ4lZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить собственную имплементацию сверточной модели на бейзлайне"
      ],
      "metadata": {
        "id": "TWqe1rFxN4fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_cnn_model(\n",
        "    [\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 32,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 64,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 128,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "    ],\n",
        "    num_classes=len(categories)\n",
        ")"
      ],
      "metadata": {
        "id": "OzhsR5hqOB1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zIAaVXyKgYO",
        "outputId": "1b5480a4-2826-4635-da70-d06e686881b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=1.3210820593535755\n",
            "Eval results: accuracy:0.2927, precision:0.1388, recall:0.1927, f1:0.1614\n",
            "Step [129 / 129], loss=1.0382257098430463\n",
            "Eval results: accuracy:0.2795, precision:0.1177, recall:0.1795, f1:0.1421\n",
            "Step [129 / 129], loss=0.8795803132244412\n",
            "Eval results: accuracy:0.3295, precision:0.2218, recall:0.2195, f1:0.2206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, попробуем обучить собственную имплементацию сверточной модели на улучшенном бейзлайне"
      ],
      "metadata": {
        "id": "-FPm1XWLPXl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_cnn_model(\n",
        "    [\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 32,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 64,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "        {\n",
        "            'kernel_size': 3,\n",
        "            'channels': 128,\n",
        "            'use_relu': True,\n",
        "            'pool_size': 2,\n",
        "        },\n",
        "    ],\n",
        "    num_classes=len(categories)\n",
        ")"
      ],
      "metadata": {
        "id": "BxNrgWgGPdwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycAHxubHPfhO",
        "outputId": "96961cc3-5a57-401a-84f0-cf54bc259d32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=1.2327465322256936\n",
            "Eval results: accuracy:0.2854, precision:0.1677, recall:0.4854, f1:0.2493\n",
            "Step [129 / 129], loss=0.6701515680138374\n",
            "Eval results: accuracy:0.4781, precision:0.2151, recall:0.4781, f1:0.2967\n",
            "Step [129 / 129], loss=0.2041168628938563\n",
            "Eval results: accuracy:0.6046, precision:0.2555, recall:0.5046, f1:0.3392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видно, собственная имплементация серточной сети показывает лучше результаты на улучшенном бейзлайне, что в очередной раз потверждает эффективность выбранных гипотез.\n",
        "\n",
        "Однако, по сравнению с моделью ResNet18 собственная имплементация показывает значительно зуже результаты. Возможной причиной может послужить то, что модель ResNet18 уже была обучена на богатом датасете ImageNet. Как следствие, она содержит богатые и универсальные признаки, по сравнению с собственной имплементацией, которая обучалась с нуля.\n",
        "\n",
        "Также возможной причиной может послужить то, что собственная имплементация не такая глубокая и не имеет такую сложную архитектуру, как модель ResNet18."
      ],
      "metadata": {
        "id": "Pv7HwdkLTOJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Трансформерная модель"
      ],
      "metadata": {
        "id": "Wlz4q74jT1o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пеейдем теперь к собственной имплементации трансформерной модели для решения задачи классификации. Вместо ООП подхода я, как и в прошлой имплементации, решил воспользоваться функциональным подходом и реализовал логику построения модели в виде функции `build_vit`, которая строит трансформерную модель.\n",
        "\n",
        "Также, для реализации слоев, выполняющих произвольные действия над входным вектором, был реализован класс `LambdaLayer`, который играет роль обертки для входной функции."
      ],
      "metadata": {
        "id": "ShgnzAN1T_R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LambdaLayer(torch.nn.Module):\n",
        "    def __init__(self, f):\n",
        "        super().__init__()\n",
        "        self._f = f\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._f(x)\n",
        "\n",
        "\n",
        "def build_vit(\n",
        "    image_size: int,\n",
        "    num_classes: int,\n",
        "    *,\n",
        "    patch_size: int = 16,\n",
        "    embedding_dimension: int = 128,\n",
        "    depth: int = 6,\n",
        "    heads_amount: int = 8,\n",
        "    mlp_ratio: int = 4,\n",
        "):\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    patch_embed = torch.nn.Conv2d(\n",
        "        3, embedding_dimension,\n",
        "        kernel_size=patch_size,\n",
        "        stride=patch_size,\n",
        "    )\n",
        "\n",
        "    cls_token = torch.nn.Parameter(torch.zeros(1, 1, embedding_dimension))\n",
        "    pos_embed = torch.nn.Parameter(torch.zeros(1, num_patches + 1, embedding_dimension))\n",
        "\n",
        "    torch.nn.init.trunc_normal_(pos_embed, std=0.02)\n",
        "    torch.nn.init.trunc_normal_(cls_token, std=0.02)\n",
        "\n",
        "    encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "        d_model=embedding_dimension,\n",
        "        nhead=heads_amount,\n",
        "        dim_feedforward=embedding_dimension * mlp_ratio,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "    head = torch.nn.Linear(embedding_dimension, num_classes)\n",
        "\n",
        "    def init_weights(m):\n",
        "        if isinstance(m, torch.nn.Linear):\n",
        "            torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "        elif isinstance(m, torch.nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "    model = torch.nn.Sequential(\n",
        "        patch_embed,\n",
        "        LambdaLayer(lambda x: x.flatten(2).transpose(1, 2)),\n",
        "        LambdaLayer(lambda x: torch.cat([cls_token.expand(x.size(0), -1, -1), x], dim=1)),\n",
        "        LambdaLayer(lambda x: x + pos_embed),\n",
        "        transformer,\n",
        "        LambdaLayer(lambda x: x[:, 0]),\n",
        "        head,\n",
        "    )\n",
        "\n",
        "    model.apply(init_weights)\n",
        "    model.cls_token = cls_token\n",
        "    model.pos_embed = pos_embed\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nVPvQNIzUAIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить собственную имплементацию трансформерной модели на бейзлайне"
      ],
      "metadata": {
        "id": "6wYgxnKTUyIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(train_dataset, categories)\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = build_vit(\n",
        "    image_size=224,\n",
        "    num_classes=len(categories),\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lniBF3pOUlp4",
        "outputId": "ee21e950-4890-4cba-d810-643f145b8c66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=1.1028448847735097\n",
            "Eval results: accuracy:0.1752, precision:0.1202, recall:0.2252, f1:0.1387\n",
            "Step [129 / 129], loss=0.7279842406063487\n",
            "Eval results: accuracy:0.3085, precision:0.2923, recall:0.2785, f1:0.2852\n",
            "Step [129 / 129], loss=0.5034145125242564\n",
            "Eval results: accuracy:0.3854, precision:0.2153, recall:0.2854, f1:0.2454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, попробуем обучить модель на улучшенном бейзлайне"
      ],
      "metadata": {
        "id": "As8Fy7KVXQNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ClassificationDataset(\n",
        "    train_dataset, categories,\n",
        "    transform=lambda image, bbox: augmentate_image(crop_image(image, bbox)),\n",
        ")\n",
        "\n",
        "val_ds = ClassificationDataset(val_dataset, categories, transform=crop_image)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "model = build_vit(\n",
        "    image_size=224,\n",
        "    num_classes=len(categories),\n",
        ")\n",
        "model.to(DEVICE)\n",
        "\n",
        "for _ in range(3):\n",
        "    train_logs = train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        torch.nn.CrossEntropyLoss(),\n",
        "        torch.optim.Adam(model.parameters(), lr=1e-3),\n",
        "    )\n",
        "\n",
        "    for log in train_logs:\n",
        "        print(\n",
        "            '\\rStep [{step} / {steps_amount}], loss={loss}'.format(\n",
        "                step=log['step'],\n",
        "                steps_amount=len(train_loader),\n",
        "                loss=log['loss'],\n",
        "            ),\n",
        "            end='',\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "    results = eval_model(model, val_loader, metrics)\n",
        "    print('Eval results: {}'.format(\n",
        "        ', '.join(f\"{name}:{value:.4f}\" for name, value in results.items())\n",
        "    ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgnIF3TAXSpX",
        "outputId": "e5c13349-8b94-46a1-deee-08b21b35c374"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [129 / 129], loss=0.671275938221748\n",
            "Eval results: accuracy:0.1927, precision:0.1786, recall:0.1927, f1:0.1853\n",
            "Step [129 / 129], loss=0.2840965115810213\n",
            "Eval results: accuracy:0.3927, precision:0.2286, recall:0.3148, f1:0.2648\n",
            "Step [129 / 129], loss=0.1564221018792586\n",
            "Eval results: accuracy:0.6527, precision:0.3862, recall:0.3927, f1:0.3894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, собственная имплементация показала результаты, ниже результатов предобученной модели ViT. Возможной причиной может быть упрощенная архитектура модели и то, что собственная имплементация не была предобученной в отличие от ViT.\n",
        "\n",
        "Однако, по сравнению с собственной имплементацией сверточной сети трансформерная модели показывает более высокие результаты, что указывает на эффективность использования трансформерных моделей для решения задачи классификации."
      ],
      "metadata": {
        "id": "ybwKI_T-bCbJ"
      }
    }
  ]
}